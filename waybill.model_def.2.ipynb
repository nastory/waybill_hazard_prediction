{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition 2\n",
    "\n",
    "In this model, we're going to try feeding the entire dataset into the 1D CNN for training, to see if there was any information lost in only feeding in the route data.\n",
    "\n",
    "Let's set things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "import plaidml.keras as pk\n",
    "pk.install_backend()\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Conv1D, Dropout, LeakyReLU, MaxPooling1D, Embedding, Flatten, Input, Concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./sequence_data.pickle', 'rb') as f:\n",
    "    sequence_data = pickle.load(f)\n",
    "    \n",
    "with open('./numerical_data.pickle', 'rb') as f:\n",
    "    numeric_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seq, y = sequence_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num, y = numeric_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68486, 5)\n",
      "(68486, 8)\n",
      "(68486,)\n"
     ]
    }
   ],
   "source": [
    "print(X_seq.shape)\n",
    "print(X_num.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([X_num, X_seq], axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_train, xs_test, ys_train, ys_test = train_test_split(X_seq, y, test_size=.33)\n",
    "xn_train, xn_test, yn_train, yn_test = train_test_split(X_num, y, test_size=.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepLearning Methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 300\n",
    "max_length = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convolutional NN Updated\n",
    "\n",
    "The baseline logistic regressor from version 1 of model_def performed better than our baseline CNN, so I'm going to update it's features to include the numerical data, since I believe that the logistic regressor's higher performance was due to these data being included.\n",
    "\n",
    "I'll update the input length to be 13, 5 for the route data + 8 for the numerical data, then use x_train and x_test, rather than xs_train and xs_test, which only contained sequence data. This will also allow for a second convolving layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Opening device \"metal_amd_radeon_pro_570x.0\"\n"
     ]
    }
   ],
   "source": [
    "cnn = Sequential()\n",
    "\n",
    "cnn.add(Embedding(vocab_size, 3, input_length=13))\n",
    "\n",
    "cnn.add(Conv1D(128, kernel_size=3, strides=1))\n",
    "cnn.add(LeakyReLU())\n",
    "cnn.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "cnn.add(Conv1D(256, kernel_size=3, strides=1))\n",
    "cnn.add(LeakyReLU())\n",
    "cnn.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "cnn.add(Flatten())\n",
    "\n",
    "cnn.add(Dense(1024))\n",
    "cnn.add(LeakyReLU())\n",
    "cnn.add(Dropout(.5))\n",
    "\n",
    "cnn.add(Dense(512))\n",
    "cnn.add(LeakyReLU())\n",
    "cnn.add(Dropout(.5))\n",
    "\n",
    "cnn.add(Dense(256))\n",
    "cnn.add(LeakyReLU())\n",
    "cnn.add(Dropout(.5))\n",
    "\n",
    "cnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "cnn.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, \n",
    "                                            verbose=2, factor=0.5, min_lr=0.00001)\n",
    "\n",
    "best_model = ModelCheckpoint('./cnn.2.1.h5', monitor='val_acc', verbose=2, \n",
    "                             save_best_only=True, mode='max')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=1e-10, \n",
    "                               patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45885 samples, validate on 22601 samples\n",
      "Epoch 1/50\n",
      "45885/45885 [==============================] - 13s 286us/step - loss: 0.3917 - acc: 0.8391 - val_loss: 0.3390 - val_acc: 0.8616\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.86155, saving model to ./cnn.2.1.h5\n",
      "Epoch 2/50\n",
      "45885/45885 [==============================] - 10s 210us/step - loss: 0.3370 - acc: 0.8692 - val_loss: 0.3200 - val_acc: 0.8717\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.86155 to 0.87173, saving model to ./cnn.2.1.h5\n",
      "Epoch 3/50\n",
      "45885/45885 [==============================] - 10s 227us/step - loss: 0.3219 - acc: 0.8759 - val_loss: 0.3436 - val_acc: 0.8603\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.87173\n",
      "Epoch 4/50\n",
      "45885/45885 [==============================] - 11s 247us/step - loss: 0.3171 - acc: 0.8787 - val_loss: 0.3217 - val_acc: 0.8714\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.87173\n",
      "Epoch 5/50\n",
      "45885/45885 [==============================] - 11s 243us/step - loss: 0.3122 - acc: 0.8812 - val_loss: 0.3112 - val_acc: 0.8725\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.87173 to 0.87248, saving model to ./cnn.2.1.h5\n",
      "Epoch 6/50\n",
      "45885/45885 [==============================] - 11s 246us/step - loss: 0.3092 - acc: 0.8821 - val_loss: 0.3302 - val_acc: 0.8792\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.87248 to 0.87916, saving model to ./cnn.2.1.h5\n",
      "Epoch 7/50\n",
      "45885/45885 [==============================] - 11s 244us/step - loss: 0.3064 - acc: 0.8831 - val_loss: 0.2998 - val_acc: 0.8821\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.87916 to 0.88208, saving model to ./cnn.2.1.h5\n",
      "Epoch 8/50\n",
      "45885/45885 [==============================] - 11s 248us/step - loss: 0.3048 - acc: 0.8842 - val_loss: 0.3035 - val_acc: 0.8828\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.88208 to 0.88284, saving model to ./cnn.2.1.h5\n",
      "Epoch 9/50\n",
      "45885/45885 [==============================] - 11s 246us/step - loss: 0.3013 - acc: 0.8851 - val_loss: 0.3092 - val_acc: 0.8692\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.88284\n",
      "Epoch 10/50\n",
      "45885/45885 [==============================] - 11s 243us/step - loss: 0.2999 - acc: 0.8860 - val_loss: 0.3003 - val_acc: 0.8856\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.88284 to 0.88562, saving model to ./cnn.2.1.h5\n",
      "Epoch 11/50\n",
      "45885/45885 [==============================] - 12s 263us/step - loss: 0.2981 - acc: 0.8859 - val_loss: 0.3250 - val_acc: 0.8809\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.88562\n",
      "Epoch 12/50\n",
      "45885/45885 [==============================] - 11s 250us/step - loss: 0.2982 - acc: 0.8874 - val_loss: 0.2935 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.88562 to 0.88607, saving model to ./cnn.2.1.h5\n",
      "Epoch 13/50\n",
      "45885/45885 [==============================] - 11s 244us/step - loss: 0.2959 - acc: 0.8874 - val_loss: 0.2926 - val_acc: 0.8856\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.88607\n",
      "Epoch 14/50\n",
      "45885/45885 [==============================] - 11s 245us/step - loss: 0.2951 - acc: 0.8880 - val_loss: 0.2932 - val_acc: 0.8862\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.88607 to 0.88620, saving model to ./cnn.2.1.h5\n",
      "Epoch 15/50\n",
      "45885/45885 [==============================] - 12s 267us/step - loss: 0.2923 - acc: 0.8891 - val_loss: 0.2947 - val_acc: 0.8847\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.88620\n",
      "Epoch 16/50\n",
      "45885/45885 [==============================] - 11s 248us/step - loss: 0.2933 - acc: 0.8890 - val_loss: 0.2943 - val_acc: 0.8867\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.88620 to 0.88669, saving model to ./cnn.2.1.h5\n",
      "Epoch 17/50\n",
      "45885/45885 [==============================] - 11s 242us/step - loss: 0.2907 - acc: 0.8904 - val_loss: 0.2967 - val_acc: 0.8845\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.88669\n",
      "Epoch 18/50\n",
      "45885/45885 [==============================] - 11s 248us/step - loss: 0.2904 - acc: 0.8885 - val_loss: 0.2877 - val_acc: 0.8897\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.88669 to 0.88974, saving model to ./cnn.2.1.h5\n",
      "Epoch 19/50\n",
      "45885/45885 [==============================] - 11s 240us/step - loss: 0.2884 - acc: 0.8899 - val_loss: 0.3193 - val_acc: 0.8817\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.88974\n",
      "Epoch 20/50\n",
      "45885/45885 [==============================] - 11s 244us/step - loss: 0.2887 - acc: 0.8895 - val_loss: 0.2908 - val_acc: 0.8891\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.88974\n",
      "Epoch 21/50\n",
      "45885/45885 [==============================] - 12s 253us/step - loss: 0.2875 - acc: 0.8912 - val_loss: 0.2872 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.88974\n",
      "Epoch 22/50\n",
      "45885/45885 [==============================] - 11s 239us/step - loss: 0.2767 - acc: 0.8937 - val_loss: 0.2833 - val_acc: 0.8915\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.88974 to 0.89146, saving model to ./cnn.2.1.h5\n",
      "Epoch 23/50\n",
      "45885/45885 [==============================] - 11s 242us/step - loss: 0.2735 - acc: 0.8946 - val_loss: 0.2816 - val_acc: 0.8910\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.89146\n",
      "Epoch 24/50\n",
      "45885/45885 [==============================] - 11s 237us/step - loss: 0.2727 - acc: 0.8946 - val_loss: 0.2823 - val_acc: 0.8887\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.89146\n",
      "Epoch 25/50\n",
      "45885/45885 [==============================] - 11s 238us/step - loss: 0.2729 - acc: 0.8943 - val_loss: 0.2778 - val_acc: 0.8908\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.89146\n",
      "Epoch 26/50\n",
      "45885/45885 [==============================] - 11s 241us/step - loss: 0.2664 - acc: 0.8969 - val_loss: 0.2783 - val_acc: 0.8925\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.89146 to 0.89248, saving model to ./cnn.2.1.h5\n",
      "Epoch 27/50\n",
      "45885/45885 [==============================] - 11s 240us/step - loss: 0.2659 - acc: 0.8975 - val_loss: 0.2778 - val_acc: 0.8920\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.89248\n",
      "Epoch 28/50\n",
      "45885/45885 [==============================] - 11s 239us/step - loss: 0.2643 - acc: 0.8977 - val_loss: 0.2762 - val_acc: 0.8931\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.89248 to 0.89306, saving model to ./cnn.2.1.h5\n",
      "Epoch 29/50\n",
      "45885/45885 [==============================] - 11s 242us/step - loss: 0.2631 - acc: 0.8974 - val_loss: 0.2776 - val_acc: 0.8919\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.89306\n",
      "Epoch 30/50\n",
      "45885/45885 [==============================] - 11s 237us/step - loss: 0.2631 - acc: 0.8980 - val_loss: 0.2774 - val_acc: 0.8924\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.89306\n",
      "Epoch 31/50\n",
      "45885/45885 [==============================] - 11s 242us/step - loss: 0.2626 - acc: 0.8979 - val_loss: 0.2760 - val_acc: 0.8912\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.89306\n",
      "Epoch 32/50\n",
      "45885/45885 [==============================] - 11s 247us/step - loss: 0.2581 - acc: 0.8994 - val_loss: 0.2755 - val_acc: 0.8930\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.89306\n",
      "Epoch 33/50\n",
      "45885/45885 [==============================] - 11s 246us/step - loss: 0.2582 - acc: 0.8993 - val_loss: 0.2748 - val_acc: 0.8931\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.89306 to 0.89310, saving model to ./cnn.2.1.h5\n",
      "Epoch 34/50\n",
      "45885/45885 [==============================] - 12s 260us/step - loss: 0.2571 - acc: 0.8994 - val_loss: 0.2759 - val_acc: 0.8937\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.89310 to 0.89368, saving model to ./cnn.2.1.h5\n",
      "Epoch 35/50\n",
      "45885/45885 [==============================] - 11s 247us/step - loss: 0.2569 - acc: 0.8996 - val_loss: 0.2778 - val_acc: 0.8929\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.89368\n",
      "Epoch 36/50\n",
      "45885/45885 [==============================] - 12s 252us/step - loss: 0.2568 - acc: 0.8998 - val_loss: 0.2765 - val_acc: 0.8940\n",
      "\n",
      "Epoch 00036: val_acc improved from 0.89368 to 0.89399, saving model to ./cnn.2.1.h5\n",
      "Epoch 37/50\n",
      "45885/45885 [==============================] - 11s 249us/step - loss: 0.2573 - acc: 0.8995 - val_loss: 0.2754 - val_acc: 0.8932\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.89399\n",
      "Epoch 38/50\n",
      "45885/45885 [==============================] - 11s 249us/step - loss: 0.2566 - acc: 0.8996 - val_loss: 0.2772 - val_acc: 0.8929\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.89399\n"
     ]
    }
   ],
   "source": [
    "hist = cnn.fit(x_train, y_train,\n",
    "         batch_size=batch_size,\n",
    "         epochs=50,\n",
    "         validation_data=(x_test, y_test),\n",
    "         callbacks=[learning_rate_reduction,best_model,early_stopping],\n",
    "         verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see marked improvement from 87.7% validation accuracy in version 1, to  89.4% validation accuracy in this version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Model\n",
    "\n",
    "We will update our combined model to include the update to our CNN and see how the performance is impacted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "\n",
    "    # cnn\n",
    "    seq_input = Input(shape=(13,))\n",
    "\n",
    "    x = Embedding(vocab_size, 3, input_length=13)(seq_input)\n",
    "\n",
    "    x = Conv1D(256, kernel_size=3, strides=1)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    x = Conv1D(512, kernel_size=3, strides=1)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(1024)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(.5)(x)\n",
    "\n",
    "    x = Dense(1024)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(.5)(x)\n",
    "\n",
    "    x = Dense(512)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(.5)(x)\n",
    "    \n",
    "    x = Dense(512)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(.5)(x)\n",
    "\n",
    "    seq_output = Dense(64, activation='relu')(x)\n",
    "\n",
    "    cnn = Model(inputs=seq_input, outputs=seq_output)\n",
    "\n",
    "    # mlp\n",
    "    num_input = Input(shape=(8,))\n",
    "    \n",
    "    y = Dense(64)(num_input)\n",
    "    y = LeakyReLU()(y)\n",
    "    y = Dropout(.5)(y)\n",
    "\n",
    "    y = Dense(1024)(y)\n",
    "    y = LeakyReLU()(y)\n",
    "    y = Dropout(.5)(y)\n",
    "\n",
    "    y = Dense(1024)(y)\n",
    "    y = LeakyReLU()(y)\n",
    "    y = Dropout(.5)(y)\n",
    "\n",
    "    y = Dense(512)(y)\n",
    "    y = LeakyReLU()(y)\n",
    "    y = Dropout(.5)(y)\n",
    "\n",
    "    mlp_output = Dense(64, activation='relu')(y)\n",
    "\n",
    "    mlp = Model(inputs=num_input, outputs=mlp_output)\n",
    "\n",
    "    # combine\n",
    "    combined = Concatenate()([cnn.output, mlp.output])\n",
    "    \n",
    "    z = Dense(512)(combined)\n",
    "    z = LeakyReLU()(z)\n",
    "    z = Dropout(.5)(z)\n",
    "    \n",
    "    z = Dense(512)(z)\n",
    "    z = LeakyReLU()(z)\n",
    "    z = Dropout(.2)(z)\n",
    "    \n",
    "    z = Dense(256)(z)\n",
    "    z = LeakyReLU()(z)\n",
    "    z = Dropout(.2)(z)\n",
    "    \n",
    "    z = Dense(64)(z)\n",
    "    z = LeakyReLU()(z)\n",
    "    z = Dropout(.2)(z)\n",
    "    \n",
    "    output = Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "    final_model = Model(inputs=mlp.inputs + cnn.inputs, outputs=[output])\n",
    "\n",
    "    final_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = x_train[:, :8]\n",
    "seq_train = x_train  # [:, 8:]\n",
    "\n",
    "num_test = x_test[:, :8]\n",
    "seq_test = x_test  #[:, 8:]\n",
    "\n",
    "xc_train = [num_train, seq_train] \n",
    "xc_test = [num_test, seq_test] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction_combined = ReduceLROnPlateau(monitor='val_acc', patience=3, \n",
    "                                            verbose=2, factor=0.5, min_lr=0.00001)\n",
    "\n",
    "best_model_combined = ModelCheckpoint('./combined_cnn_mlp_model.2.1.h5', monitor='val_acc', verbose=2, \n",
    "                             save_best_only=True, mode='max')\n",
    "\n",
    "early_stopping_combined = EarlyStopping(monitor='val_loss', min_delta=1e-10, \n",
    "                               patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45885 samples, validate on 22601 samples\n",
      "Epoch 1/50\n",
      "45885/45885 [==============================] - 34s 743us/step - loss: 0.3839 - acc: 0.8728 - val_loss: 0.3463 - val_acc: 0.8843\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.88425, saving model to ./combined_cnn_mlp_model.2.1.h5\n",
      "Epoch 2/50\n",
      "45885/45885 [==============================] - 27s 583us/step - loss: 0.3382 - acc: 0.8876 - val_loss: 0.3968 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.88425\n",
      "Epoch 3/50\n",
      "45885/45885 [==============================] - 27s 579us/step - loss: 0.3259 - acc: 0.8916 - val_loss: 0.2967 - val_acc: 0.8943\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.88425 to 0.89434, saving model to ./combined_cnn_mlp_model.2.1.h5\n",
      "Epoch 4/50\n",
      "45885/45885 [==============================] - 26s 575us/step - loss: 0.3189 - acc: 0.8935 - val_loss: 0.3108 - val_acc: 0.8884\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.89434\n",
      "Epoch 5/50\n",
      "45885/45885 [==============================] - 27s 582us/step - loss: 0.3209 - acc: 0.8944 - val_loss: 0.3343 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.89434\n",
      "Epoch 6/50\n",
      "45885/45885 [==============================] - 29s 629us/step - loss: 0.3161 - acc: 0.8951 - val_loss: 0.3057 - val_acc: 0.8930\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.89434\n",
      "Epoch 7/50\n",
      "45885/45885 [==============================] - 29s 623us/step - loss: 0.2970 - acc: 0.8989 - val_loss: 0.2941 - val_acc: 0.8945\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.89434 to 0.89447, saving model to ./combined_cnn_mlp_model.2.1.h5\n",
      "Epoch 8/50\n",
      "45885/45885 [==============================] - 29s 626us/step - loss: 0.2942 - acc: 0.8990 - val_loss: 0.2887 - val_acc: 0.8965\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.89447 to 0.89646, saving model to ./combined_cnn_mlp_model.2.1.h5\n",
      "Epoch 9/50\n",
      "45885/45885 [==============================] - 29s 624us/step - loss: 0.2923 - acc: 0.8999 - val_loss: 0.2900 - val_acc: 0.8975\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.89646 to 0.89753, saving model to ./combined_cnn_mlp_model.2.1.h5\n",
      "Epoch 10/50\n",
      "45885/45885 [==============================] - 29s 624us/step - loss: 0.2927 - acc: 0.9000 - val_loss: 0.2853 - val_acc: 0.8977\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.89753 to 0.89770, saving model to ./combined_cnn_mlp_model.2.1.h5\n",
      "Epoch 11/50\n",
      "45885/45885 [==============================] - 28s 618us/step - loss: 0.2935 - acc: 0.9009 - val_loss: 0.2939 - val_acc: 0.8975\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.89770\n",
      "Epoch 12/50\n",
      "45885/45885 [==============================] - 29s 626us/step - loss: 0.2867 - acc: 0.9003 - val_loss: 0.2920 - val_acc: 0.8979\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.89770 to 0.89792, saving model to ./combined_cnn_mlp_model.2.1.h5\n",
      "Epoch 13/50\n",
      "45885/45885 [==============================] - 28s 620us/step - loss: 0.2887 - acc: 0.9018 - val_loss: 0.2846 - val_acc: 0.8971\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.89792\n",
      "Epoch 14/50\n",
      "45885/45885 [==============================] - 29s 627us/step - loss: 0.2861 - acc: 0.9021 - val_loss: 0.2834 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.89792 to 0.89996, saving model to ./combined_cnn_mlp_model.2.1.h5\n",
      "Epoch 15/50\n",
      "45885/45885 [==============================] - 29s 631us/step - loss: 0.2843 - acc: 0.9026 - val_loss: 0.2876 - val_acc: 0.8991\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.89996\n",
      "Epoch 16/50\n",
      "45885/45885 [==============================] - 29s 621us/step - loss: 0.2841 - acc: 0.9023 - val_loss: 0.2842 - val_acc: 0.8969\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.89996\n",
      "Epoch 17/50\n",
      "45885/45885 [==============================] - 28s 620us/step - loss: 0.2822 - acc: 0.9037 - val_loss: 0.2977 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.89996\n",
      "Epoch 18/50\n",
      "45885/45885 [==============================] - 28s 611us/step - loss: 0.2730 - acc: 0.9054 - val_loss: 0.2822 - val_acc: 0.9014\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.89996 to 0.90142, saving model to ./combined_cnn_mlp_model.2.1.h5\n",
      "Epoch 19/50\n",
      "45885/45885 [==============================] - 28s 609us/step - loss: 0.2744 - acc: 0.9056 - val_loss: 0.2750 - val_acc: 0.9023\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.90142 to 0.90231, saving model to ./combined_cnn_mlp_model.2.1.h5\n",
      "Epoch 20/50\n",
      "45885/45885 [==============================] - 28s 616us/step - loss: 0.2721 - acc: 0.9051 - val_loss: 0.2764 - val_acc: 0.9009\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.90231\n",
      "Epoch 21/50\n",
      "45885/45885 [==============================] - 28s 611us/step - loss: 0.2721 - acc: 0.9059 - val_loss: 0.2807 - val_acc: 0.9015\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.90231\n",
      "Epoch 22/50\n",
      "45885/45885 [==============================] - 28s 616us/step - loss: 0.2694 - acc: 0.9063 - val_loss: 0.2792 - val_acc: 0.9006\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.90231\n",
      "Epoch 23/50\n",
      "45885/45885 [==============================] - 28s 616us/step - loss: 0.2639 - acc: 0.9073 - val_loss: 0.2719 - val_acc: 0.9032\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.90231 to 0.90323, saving model to ./combined_cnn_mlp_model.2.1.h5\n",
      "Epoch 24/50\n",
      "45885/45885 [==============================] - 28s 617us/step - loss: 0.2630 - acc: 0.9069 - val_loss: 0.2738 - val_acc: 0.9026\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.90323\n",
      "Epoch 25/50\n",
      "45885/45885 [==============================] - 28s 606us/step - loss: 0.2630 - acc: 0.9077 - val_loss: 0.2731 - val_acc: 0.9028\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.90323\n",
      "Epoch 26/50\n",
      "45885/45885 [==============================] - 28s 600us/step - loss: 0.2614 - acc: 0.9077 - val_loss: 0.2709 - val_acc: 0.9029\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.90323\n",
      "Epoch 27/50\n",
      "45885/45885 [==============================] - 28s 612us/step - loss: 0.2595 - acc: 0.9083 - val_loss: 0.2696 - val_acc: 0.9043\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.90323 to 0.90430, saving model to ./combined_cnn_mlp_model.2.1.h5\n",
      "Epoch 28/50\n",
      "45885/45885 [==============================] - 29s 624us/step - loss: 0.2586 - acc: 0.9088 - val_loss: 0.2696 - val_acc: 0.9042\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.90430\n",
      "Epoch 29/50\n",
      "45885/45885 [==============================] - 28s 615us/step - loss: 0.2565 - acc: 0.9091 - val_loss: 0.2688 - val_acc: 0.9044\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.90430 to 0.90438, saving model to ./combined_cnn_mlp_model.2.1.h5\n",
      "Epoch 30/50\n",
      "45885/45885 [==============================] - 28s 609us/step - loss: 0.2570 - acc: 0.9093 - val_loss: 0.2687 - val_acc: 0.9046\n",
      "\n",
      "Epoch 00030: val_acc improved from 0.90438 to 0.90456, saving model to ./combined_cnn_mlp_model.2.1.h5\n",
      "Epoch 31/50\n",
      "45885/45885 [==============================] - 28s 605us/step - loss: 0.2564 - acc: 0.9089 - val_loss: 0.2706 - val_acc: 0.9043\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.90456\n",
      "Epoch 32/50\n",
      "45885/45885 [==============================] - 29s 643us/step - loss: 0.2569 - acc: 0.9089 - val_loss: 0.2719 - val_acc: 0.9040\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.90456\n",
      "Epoch 33/50\n",
      "45885/45885 [==============================] - 29s 632us/step - loss: 0.2566 - acc: 0.9092 - val_loss: 0.2681 - val_acc: 0.9048\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.90456 to 0.90478, saving model to ./combined_cnn_mlp_model.2.1.h5\n",
      "Epoch 34/50\n",
      "45885/45885 [==============================] - 30s 659us/step - loss: 0.2563 - acc: 0.9092 - val_loss: 0.2691 - val_acc: 0.9040\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.90478\n",
      "Epoch 35/50\n",
      "45885/45885 [==============================] - 30s 648us/step - loss: 0.2562 - acc: 0.9094 - val_loss: 0.2688 - val_acc: 0.9050\n",
      "\n",
      "Epoch 00035: val_acc improved from 0.90478 to 0.90505, saving model to ./combined_cnn_mlp_model.2.1.h5\n",
      "Epoch 36/50\n",
      "45885/45885 [==============================] - 30s 647us/step - loss: 0.2553 - acc: 0.9094 - val_loss: 0.2691 - val_acc: 0.9048\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.90505\n",
      "Epoch 37/50\n",
      "45885/45885 [==============================] - 29s 641us/step - loss: 0.2553 - acc: 0.9096 - val_loss: 0.2683 - val_acc: 0.9055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00037: val_acc improved from 0.90505 to 0.90554, saving model to ./combined_cnn_mlp_model.2.1.h5\n",
      "Epoch 38/50\n",
      "45885/45885 [==============================] - 30s 655us/step - loss: 0.2553 - acc: 0.9096 - val_loss: 0.2693 - val_acc: 0.9047\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.90554\n",
      "Epoch 39/50\n",
      "45885/45885 [==============================] - 30s 647us/step - loss: 0.2543 - acc: 0.9095 - val_loss: 0.2669 - val_acc: 0.9051\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.90554\n",
      "Epoch 40/50\n",
      "45885/45885 [==============================] - 30s 664us/step - loss: 0.2539 - acc: 0.9099 - val_loss: 0.2702 - val_acc: 0.9050\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.90554\n",
      "Epoch 41/50\n",
      "45885/45885 [==============================] - 31s 666us/step - loss: 0.2530 - acc: 0.9102 - val_loss: 0.2692 - val_acc: 0.9056\n",
      "\n",
      "Epoch 00041: val_acc improved from 0.90554 to 0.90558, saving model to ./combined_cnn_mlp_model.2.1.h5\n",
      "Epoch 42/50\n",
      "45885/45885 [==============================] - 30s 661us/step - loss: 0.2522 - acc: 0.9105 - val_loss: 0.2680 - val_acc: 0.9054\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.90558\n",
      "Epoch 43/50\n",
      "45885/45885 [==============================] - 31s 667us/step - loss: 0.2523 - acc: 0.9105 - val_loss: 0.2687 - val_acc: 0.9056\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 00043: val_acc improved from 0.90558 to 0.90562, saving model to ./combined_cnn_mlp_model.2.1.h5\n",
      "Epoch 44/50\n",
      "45885/45885 [==============================] - 32s 693us/step - loss: 0.2511 - acc: 0.9108 - val_loss: 0.2679 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00044: val_acc improved from 0.90562 to 0.90571, saving model to ./combined_cnn_mlp_model.2.1.h5\n",
      "Epoch 45/50\n",
      "45885/45885 [==============================] - 30s 659us/step - loss: 0.2501 - acc: 0.9108 - val_loss: 0.2686 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.90571\n",
      "Epoch 46/50\n",
      "45885/45885 [==============================] - 31s 683us/step - loss: 0.2505 - acc: 0.9106 - val_loss: 0.2680 - val_acc: 0.9058\n",
      "\n",
      "Epoch 00046: val_acc improved from 0.90571 to 0.90576, saving model to ./combined_cnn_mlp_model.2.1.h5\n",
      "Epoch 47/50\n",
      "45885/45885 [==============================] - 30s 657us/step - loss: 0.2512 - acc: 0.9104 - val_loss: 0.2680 - val_acc: 0.9055\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.90576\n",
      "Epoch 48/50\n",
      "45885/45885 [==============================] - 32s 699us/step - loss: 0.2505 - acc: 0.9108 - val_loss: 0.2683 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00048: val_acc improved from 0.90576 to 0.90589, saving model to ./combined_cnn_mlp_model.2.1.h5\n",
      "Epoch 49/50\n",
      "45885/45885 [==============================] - 30s 663us/step - loss: 0.2508 - acc: 0.9105 - val_loss: 0.2684 - val_acc: 0.9058\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.90589\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(xc_train, y_train,\n",
    "         batch_size=batch_size,\n",
    "         epochs=50,\n",
    "         validation_data=(xc_test, y_test),\n",
    "         callbacks = [learning_rate_reduction_combined, best_model_combined, early_stopping_combined],\n",
    "         verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the improvements to the CNN part of our model, the overall performance has decreased from 91.0% to 90.5%.\n",
    "\n",
    "Keeping in mind that our Logistic Regressor performed similarly to the MLP we built, let's try simplifying our CNN and MLP to see if, in this case, less could be more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
