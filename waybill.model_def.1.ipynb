{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition 1\n",
    "\n",
    "We'll be defining a few models here to compare: a simple random forest classifier, a logistic regressor, a 1D CNN, a multi-layer perceptron, and a combined hybrid neural network with converging CNN and MLP networks. The logistic regressor will serve as our baseline model.\n",
    "\n",
    "Let's read in our data and imports and get to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "import plaidml.keras as pk\n",
    "pk.install_backend()\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Conv1D, Dropout, LeakyReLU, MaxPooling1D, Embedding, Flatten, Input, Concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./sequence_data.pickle', 'rb') as f:\n",
    "    sequence_data = pickle.load(f)\n",
    "    \n",
    "with open('./numerical_data.pickle', 'rb') as f:\n",
    "    numeric_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seq, y = sequence_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num, y = numeric_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68486, 5)\n",
      "(68486, 8)\n",
      "(68486,)\n"
     ]
    }
   ],
   "source": [
    "print(X_seq.shape)\n",
    "print(X_num.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([X_num, X_seq], axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_train, xs_test, ys_train, ys_test = train_test_split(X_seq, y, test_size=.33)\n",
    "xn_train, xn_test, yn_train, yn_test = train_test_split(X_num, y, test_size=.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-DeepLearning Methods \n",
    "\n",
    "We'll start with our non-deeplearning models -- the random forest and logistic regressor. We'll start with the random forest.\n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "We're doing a randomized hyperparameter search over a wide parameter space. This will give a good chance of finding the best model since we get to search a wide range of hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "n_estimators = [x for x in range(1,201,10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [x for x in range(1, 101, 10)] + [None]\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "params = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_features': max_features,\n",
    "    'max_depth': max_depth,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'bootstrap': bootstrap   \n",
    "}\n",
    "\n",
    "rf_grid = RandomizedSearchCV(estimator = rf, param_distributions = params,\n",
    "                             n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:   18.1s\n",
      "[Parallel(n_jobs=-1)]: Done 150 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators='warn',\n",
       "                                                    n_jobs=None,\n",
       "                                                    oob_sc...\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [1, 11, 21, 31, 41, 51, 61,\n",
       "                                                      71, 81, 91, None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [1, 11, 21, 31, 41, 51,\n",
       "                                                         61, 71, 81, 91, 101,\n",
       "                                                         111, 121, 131, 141,\n",
       "                                                         151, 161, 171, 181,\n",
       "                                                         191]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.84%\n"
     ]
    }
   ],
   "source": [
    "best_rf = rf_grid.best_estimator_\n",
    "\n",
    "preds = best_rf.predict(x_test)\n",
    "acc = accuracy_score(preds, y_test)\n",
    "print(f'Accuracy: {round(acc*100,2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right out of the box, we get a very stong model! 92.8% accuracy is great, and given that our dataset is balanced by design, our other metrics should be around 93% as well. Let's take a look at the classification report to varify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93     11705\n",
      "           1       0.91      0.94      0.93     10896\n",
      "\n",
      "    accuracy                           0.93     22601\n",
      "   macro avg       0.93      0.93      0.93     22601\n",
      "weighted avg       0.93      0.93      0.93     22601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9287260557036948"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, precision, recall, F1, and AUC-ROC scores are all very good -- around 92%.\n",
    "\n",
    "We'll persist this model through pickling for use in further evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./random_forest_model.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rf_grid, './random_forest_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 300\n",
    "max_length = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Logistic Regressor\n",
    "\n",
    "Next we'll make a simple logistic regressor to serve as a performance baseline for our deep learning networks.\n",
    "\n",
    "We'll first make an embedding for our routes, and then pass the data through a single sigmoid layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Opening device \"metal_amd_radeon_pro_570x.0\"\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, 3, input_length=13))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45885 samples, validate on 22601 samples\n",
      "Epoch 1/25\n",
      "45885/45885 [==============================] - 8s 165us/step - loss: 0.4800 - acc: 0.7968 - val_loss: 0.3950 - val_acc: 0.8381\n",
      "Epoch 2/25\n",
      "45885/45885 [==============================] - 7s 153us/step - loss: 0.3880 - acc: 0.8422 - val_loss: 0.3782 - val_acc: 0.8457\n",
      "Epoch 3/25\n",
      "45885/45885 [==============================] - 7s 146us/step - loss: 0.3752 - acc: 0.8456 - val_loss: 0.3691 - val_acc: 0.8473\n",
      "Epoch 4/25\n",
      "45885/45885 [==============================] - 6s 140us/step - loss: 0.3676 - acc: 0.8484 - val_loss: 0.3639 - val_acc: 0.8479\n",
      "Epoch 5/25\n",
      "45885/45885 [==============================] - 7s 147us/step - loss: 0.3628 - acc: 0.8506 - val_loss: 0.3602 - val_acc: 0.8539\n",
      "Epoch 6/25\n",
      "45885/45885 [==============================] - 7s 143us/step - loss: 0.3594 - acc: 0.8542 - val_loss: 0.3575 - val_acc: 0.8562\n",
      "Epoch 7/25\n",
      "45885/45885 [==============================] - 7s 156us/step - loss: 0.3568 - acc: 0.8561 - val_loss: 0.3555 - val_acc: 0.8593\n",
      "Epoch 8/25\n",
      "45885/45885 [==============================] - 7s 147us/step - loss: 0.3549 - acc: 0.8577 - val_loss: 0.3542 - val_acc: 0.8612\n",
      "Epoch 9/25\n",
      "45885/45885 [==============================] - 7s 142us/step - loss: 0.3530 - acc: 0.8588 - val_loss: 0.3522 - val_acc: 0.8618\n",
      "Epoch 10/25\n",
      "45885/45885 [==============================] - 7s 150us/step - loss: 0.3518 - acc: 0.8605 - val_loss: 0.3514 - val_acc: 0.8590\n",
      "Epoch 11/25\n",
      "45885/45885 [==============================] - 6s 139us/step - loss: 0.3506 - acc: 0.8602 - val_loss: 0.3507 - val_acc: 0.8631\n",
      "Epoch 12/25\n",
      "45885/45885 [==============================] - 7s 149us/step - loss: 0.3497 - acc: 0.8619 - val_loss: 0.3494 - val_acc: 0.8616\n",
      "Epoch 13/25\n",
      "45885/45885 [==============================] - 7s 144us/step - loss: 0.3488 - acc: 0.8614 - val_loss: 0.3486 - val_acc: 0.8618\n",
      "Epoch 14/25\n",
      "45885/45885 [==============================] - 6s 138us/step - loss: 0.3482 - acc: 0.8608 - val_loss: 0.3484 - val_acc: 0.8612\n",
      "Epoch 15/25\n",
      "45885/45885 [==============================] - 7s 144us/step - loss: 0.3478 - acc: 0.8619 - val_loss: 0.3480 - val_acc: 0.8645\n",
      "Epoch 16/25\n",
      "45885/45885 [==============================] - 7s 142us/step - loss: 0.3473 - acc: 0.8617 - val_loss: 0.3480 - val_acc: 0.8630\n",
      "Epoch 17/25\n",
      "45885/45885 [==============================] - 7s 146us/step - loss: 0.3469 - acc: 0.8620 - val_loss: 0.3477 - val_acc: 0.8653\n",
      "Epoch 18/25\n",
      "45885/45885 [==============================] - 7s 145us/step - loss: 0.3467 - acc: 0.8621 - val_loss: 0.3474 - val_acc: 0.8602\n",
      "Epoch 19/25\n",
      "45885/45885 [==============================] - 7s 143us/step - loss: 0.3464 - acc: 0.8617 - val_loss: 0.3478 - val_acc: 0.8597\n",
      "Epoch 20/25\n",
      "45885/45885 [==============================] - 7s 147us/step - loss: 0.3461 - acc: 0.8628 - val_loss: 0.3472 - val_acc: 0.8607\n",
      "Epoch 21/25\n",
      "45885/45885 [==============================] - 7s 146us/step - loss: 0.3460 - acc: 0.8615 - val_loss: 0.3474 - val_acc: 0.8648\n",
      "Epoch 22/25\n",
      "45885/45885 [==============================] - 7s 150us/step - loss: 0.3457 - acc: 0.8619 - val_loss: 0.3471 - val_acc: 0.8614\n",
      "Epoch 23/25\n",
      "45885/45885 [==============================] - 7s 143us/step - loss: 0.3457 - acc: 0.8624 - val_loss: 0.3466 - val_acc: 0.8635\n",
      "Epoch 24/25\n",
      "45885/45885 [==============================] - 7s 144us/step - loss: 0.3455 - acc: 0.8623 - val_loss: 0.3466 - val_acc: 0.8654\n",
      "Epoch 25/25\n",
      "45885/45885 [==============================] - 7s 150us/step - loss: 0.3454 - acc: 0.8631 - val_loss: 0.3484 - val_acc: 0.8625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa979161690>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "         batch_size=batch_size,\n",
    "         epochs=25,\n",
    "         validation_data=(x_test, y_test),\n",
    "         verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regressor is scoring around 86% accuracy. This will make a fine baseline by which to judge our other models.\n",
    "\n",
    "We'll save the model weights as an h5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./logistic_regressor.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepLearning Methods:\n",
    "\n",
    "For our deep learning model we'll be implementing two separate models, a CNN and MLP model, and then we'll combine the two into a hybrid model. The reason for this approach is that we have two separate types of data to model: sequence data and numerical/categorical data. Our CNN will be modeling the sequence data, and the MLP will model our numerical and categorical data. We'll then combine outputs of those two models and feed them into a third model and see if the performance is boosted.\n",
    "\n",
    "Let's start by developing our 1D CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional NN\n",
    "\n",
    "We'll start small, with only one convolving layer going into a single hidden dense layer. The inputs will be our \"route sentences\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential()\n",
    "\n",
    "cnn.add(Embedding(vocab_size, 3, input_length=5))\n",
    "\n",
    "cnn.add(Conv1D(64, kernel_size=3, strides=1))\n",
    "cnn.add(MaxPooling1D(pool_size=2))\n",
    "cnn.add(Flatten())\n",
    "\n",
    "cnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "cnn.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45885 samples, validate on 22601 samples\n",
      "Epoch 1/25\n",
      "45885/45885 [==============================] - 9s 194us/step - loss: 0.4641 - acc: 0.7856 - val_loss: 0.4218 - val_acc: 0.8135\n",
      "Epoch 2/25\n",
      "45885/45885 [==============================] - 7s 162us/step - loss: 0.4007 - acc: 0.8329 - val_loss: 0.3904 - val_acc: 0.8383\n",
      "Epoch 3/25\n",
      "45885/45885 [==============================] - 8s 181us/step - loss: 0.3808 - acc: 0.8456 - val_loss: 0.3829 - val_acc: 0.8426\n",
      "Epoch 4/25\n",
      "45885/45885 [==============================] - 7s 163us/step - loss: 0.3732 - acc: 0.8475 - val_loss: 0.3754 - val_acc: 0.8427\n",
      "Epoch 5/25\n",
      "45885/45885 [==============================] - 7s 161us/step - loss: 0.3681 - acc: 0.8482 - val_loss: 0.3725 - val_acc: 0.8430\n",
      "Epoch 6/25\n",
      "45885/45885 [==============================] - 8s 169us/step - loss: 0.3650 - acc: 0.8491 - val_loss: 0.3715 - val_acc: 0.8410\n",
      "Epoch 7/25\n",
      "45885/45885 [==============================] - 7s 158us/step - loss: 0.3623 - acc: 0.8497 - val_loss: 0.3700 - val_acc: 0.8441\n",
      "Epoch 8/25\n",
      "45885/45885 [==============================] - 7s 163us/step - loss: 0.3607 - acc: 0.8526 - val_loss: 0.3698 - val_acc: 0.8446\n",
      "Epoch 9/25\n",
      "45885/45885 [==============================] - 8s 169us/step - loss: 0.3591 - acc: 0.8527 - val_loss: 0.3659 - val_acc: 0.8465\n",
      "Epoch 10/25\n",
      "45885/45885 [==============================] - 6s 135us/step - loss: 0.3582 - acc: 0.8530 - val_loss: 0.3639 - val_acc: 0.8479\n",
      "Epoch 11/25\n",
      "45885/45885 [==============================] - 8s 183us/step - loss: 0.3567 - acc: 0.8537 - val_loss: 0.3682 - val_acc: 0.8466\n",
      "Epoch 12/25\n",
      "45885/45885 [==============================] - 9s 203us/step - loss: 0.3564 - acc: 0.8547 - val_loss: 0.3669 - val_acc: 0.8472\n",
      "Epoch 13/25\n",
      "45885/45885 [==============================] - 8s 171us/step - loss: 0.3551 - acc: 0.8548 - val_loss: 0.3621 - val_acc: 0.8486\n",
      "Epoch 14/25\n",
      "45885/45885 [==============================] - 7s 163us/step - loss: 0.3542 - acc: 0.8559 - val_loss: 0.3612 - val_acc: 0.8478\n",
      "Epoch 15/25\n",
      "45885/45885 [==============================] - 9s 203us/step - loss: 0.3542 - acc: 0.8542 - val_loss: 0.3667 - val_acc: 0.8472\n",
      "Epoch 16/25\n",
      "45885/45885 [==============================] - 9s 201us/step - loss: 0.3530 - acc: 0.8560 - val_loss: 0.3622 - val_acc: 0.8486\n",
      "Epoch 17/25\n",
      "45885/45885 [==============================] - 10s 213us/step - loss: 0.3527 - acc: 0.8554 - val_loss: 0.3590 - val_acc: 0.8519\n",
      "Epoch 18/25\n",
      "45885/45885 [==============================] - 9s 201us/step - loss: 0.3522 - acc: 0.8557 - val_loss: 0.3616 - val_acc: 0.8496\n",
      "Epoch 19/25\n",
      "45885/45885 [==============================] - 9s 202us/step - loss: 0.3515 - acc: 0.8565 - val_loss: 0.3583 - val_acc: 0.8503\n",
      "Epoch 20/25\n",
      "45885/45885 [==============================] - 10s 211us/step - loss: 0.3511 - acc: 0.8564 - val_loss: 0.3607 - val_acc: 0.8504\n",
      "Epoch 21/25\n",
      "45885/45885 [==============================] - 9s 202us/step - loss: 0.3507 - acc: 0.8569 - val_loss: 0.3567 - val_acc: 0.8533\n",
      "Epoch 22/25\n",
      "45885/45885 [==============================] - 9s 203us/step - loss: 0.3502 - acc: 0.8571 - val_loss: 0.3584 - val_acc: 0.8505\n",
      "Epoch 23/25\n",
      "45885/45885 [==============================] - 10s 216us/step - loss: 0.3496 - acc: 0.8575 - val_loss: 0.3575 - val_acc: 0.8498\n",
      "Epoch 24/25\n",
      "45885/45885 [==============================] - 9s 206us/step - loss: 0.3494 - acc: 0.8572 - val_loss: 0.3549 - val_acc: 0.8518\n",
      "Epoch 25/25\n",
      "45885/45885 [==============================] - 9s 204us/step - loss: 0.3489 - acc: 0.8571 - val_loss: 0.3560 - val_acc: 0.8543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa949f21990>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(xs_train, ys_train,\n",
    "         batch_size=batch_size,\n",
    "         epochs=25,\n",
    "         validation_data=(xs_test, ys_test),\n",
    "         verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This CNN isn't performing very well -- hitting around 1% below the simple logistic regressor. Let's see if we can improve it through adding layers to the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save('./baseline_cnn.85acc.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improved CNN\n",
    "\n",
    "Let's take the model above, double the convolution filters, and add a few more dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential()\n",
    "\n",
    "cnn.add(Embedding(vocab_size, 3, input_length=5))\n",
    "\n",
    "cnn.add(Conv1D(128, kernel_size=3, strides=1))\n",
    "cnn.add(LeakyReLU())\n",
    "cnn.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "cnn.add(Flatten())\n",
    "\n",
    "cnn.add(Dense(1024))\n",
    "cnn.add(LeakyReLU())\n",
    "cnn.add(Dropout(.5))\n",
    "\n",
    "cnn.add(Dense(512))\n",
    "cnn.add(LeakyReLU())\n",
    "cnn.add(Dropout(.5))\n",
    "\n",
    "cnn.add(Dense(256))\n",
    "cnn.add(LeakyReLU())\n",
    "cnn.add(Dropout(.5))\n",
    "\n",
    "cnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "cnn.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, \n",
    "                                            verbose=2, factor=0.5, min_lr=0.00001)\n",
    "\n",
    "best_model = ModelCheckpoint('./cnn.2.h5', monitor='val_acc', verbose=2, \n",
    "                             save_best_only=True, mode='max')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=1e-10, \n",
    "                               patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45885 samples, validate on 22601 samples\n",
      "Epoch 1/50\n",
      "45885/45885 [==============================] - 17s 372us/step - loss: 0.4254 - acc: 0.8141 - val_loss: 0.3937 - val_acc: 0.8367\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.83669, saving model to ./cnn.2.h5\n",
      "Epoch 2/50\n",
      "45885/45885 [==============================] - 15s 323us/step - loss: 0.3783 - acc: 0.8425 - val_loss: 0.3630 - val_acc: 0.8481\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.83669 to 0.84815, saving model to ./cnn.2.h5\n",
      "Epoch 3/50\n",
      "45885/45885 [==============================] - 15s 329us/step - loss: 0.3632 - acc: 0.8524 - val_loss: 0.3650 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.84815 to 0.85151, saving model to ./cnn.2.h5\n",
      "Epoch 4/50\n",
      "45885/45885 [==============================] - 14s 299us/step - loss: 0.3579 - acc: 0.8548 - val_loss: 0.3619 - val_acc: 0.8486\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.85151\n",
      "Epoch 5/50\n",
      "45885/45885 [==============================] - 15s 327us/step - loss: 0.3528 - acc: 0.8574 - val_loss: 0.3454 - val_acc: 0.8615\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.85151 to 0.86151, saving model to ./cnn.2.h5\n",
      "Epoch 6/50\n",
      "45885/45885 [==============================] - 14s 306us/step - loss: 0.3515 - acc: 0.8607 - val_loss: 0.3612 - val_acc: 0.8593\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.86151\n",
      "Epoch 7/50\n",
      "45885/45885 [==============================] - 15s 328us/step - loss: 0.3505 - acc: 0.8597 - val_loss: 0.3513 - val_acc: 0.8602\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.86151\n",
      "Epoch 8/50\n",
      "45885/45885 [==============================] - 14s 309us/step - loss: 0.3478 - acc: 0.8623 - val_loss: 0.3544 - val_acc: 0.8567\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.86151\n",
      "Epoch 9/50\n",
      "45885/45885 [==============================] - 15s 333us/step - loss: 0.3366 - acc: 0.8673 - val_loss: 0.3385 - val_acc: 0.8624\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.86151 to 0.86240, saving model to ./cnn.2.h5\n",
      "Epoch 10/50\n",
      "45885/45885 [==============================] - 14s 300us/step - loss: 0.3348 - acc: 0.8677 - val_loss: 0.3374 - val_acc: 0.8634\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.86240 to 0.86341, saving model to ./cnn.2.h5\n",
      "Epoch 11/50\n",
      "45885/45885 [==============================] - 15s 331us/step - loss: 0.3328 - acc: 0.8687 - val_loss: 0.3370 - val_acc: 0.8637\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.86341 to 0.86368, saving model to ./cnn.2.h5\n",
      "Epoch 12/50\n",
      "45885/45885 [==============================] - 14s 303us/step - loss: 0.3313 - acc: 0.8681 - val_loss: 0.3341 - val_acc: 0.8645\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.86368 to 0.86448, saving model to ./cnn.2.h5\n",
      "Epoch 13/50\n",
      "45885/45885 [==============================] - 15s 335us/step - loss: 0.3301 - acc: 0.8693 - val_loss: 0.3354 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.86448 to 0.86518, saving model to ./cnn.2.h5\n",
      "Epoch 14/50\n",
      "45885/45885 [==============================] - 14s 296us/step - loss: 0.3296 - acc: 0.8705 - val_loss: 0.3562 - val_acc: 0.8552\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.86518\n",
      "Epoch 15/50\n",
      "45885/45885 [==============================] - 15s 333us/step - loss: 0.3285 - acc: 0.8699 - val_loss: 0.3346 - val_acc: 0.8662\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.86518 to 0.86616, saving model to ./cnn.2.h5\n",
      "Epoch 16/50\n",
      "45885/45885 [==============================] - 14s 296us/step - loss: 0.3273 - acc: 0.8711 - val_loss: 0.3409 - val_acc: 0.8597\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.86616\n",
      "Epoch 17/50\n",
      "45885/45885 [==============================] - 15s 334us/step - loss: 0.3260 - acc: 0.8723 - val_loss: 0.3321 - val_acc: 0.8653\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.86616\n",
      "Epoch 18/50\n",
      "45885/45885 [==============================] - 13s 289us/step - loss: 0.3259 - acc: 0.8723 - val_loss: 0.3325 - val_acc: 0.8645\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.86616\n",
      "Epoch 19/50\n",
      "45885/45885 [==============================] - 16s 338us/step - loss: 0.3193 - acc: 0.8752 - val_loss: 0.3279 - val_acc: 0.8683\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.86616 to 0.86828, saving model to ./cnn.2.h5\n",
      "Epoch 20/50\n",
      "45885/45885 [==============================] - 14s 299us/step - loss: 0.3195 - acc: 0.8753 - val_loss: 0.3282 - val_acc: 0.8681\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.86828\n",
      "Epoch 21/50\n",
      "45885/45885 [==============================] - 15s 333us/step - loss: 0.3175 - acc: 0.8757 - val_loss: 0.3253 - val_acc: 0.8705\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.86828 to 0.87049, saving model to ./cnn.2.h5\n",
      "Epoch 22/50\n",
      "45885/45885 [==============================] - 14s 297us/step - loss: 0.3169 - acc: 0.8766 - val_loss: 0.3275 - val_acc: 0.8697\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.87049\n",
      "Epoch 23/50\n",
      "45885/45885 [==============================] - 15s 334us/step - loss: 0.3169 - acc: 0.8770 - val_loss: 0.3265 - val_acc: 0.8710\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.87049 to 0.87102, saving model to ./cnn.2.h5\n",
      "Epoch 24/50\n",
      "45885/45885 [==============================] - 13s 292us/step - loss: 0.3173 - acc: 0.8774 - val_loss: 0.3262 - val_acc: 0.8694\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.87102\n",
      "Epoch 25/50\n",
      "45885/45885 [==============================] - 15s 331us/step - loss: 0.3160 - acc: 0.8771 - val_loss: 0.3240 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.87102 to 0.87204, saving model to ./cnn.2.h5\n",
      "Epoch 26/50\n",
      "45885/45885 [==============================] - 13s 291us/step - loss: 0.3157 - acc: 0.8768 - val_loss: 0.3235 - val_acc: 0.8714\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.87204\n",
      "Epoch 27/50\n",
      "45885/45885 [==============================] - 15s 327us/step - loss: 0.3155 - acc: 0.8775 - val_loss: 0.3241 - val_acc: 0.8721\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.87204 to 0.87209, saving model to ./cnn.2.h5\n",
      "Epoch 28/50\n",
      "45885/45885 [==============================] - 13s 291us/step - loss: 0.3139 - acc: 0.8772 - val_loss: 0.3249 - val_acc: 0.8716\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.87209\n",
      "Epoch 29/50\n",
      "45885/45885 [==============================] - 15s 332us/step - loss: 0.3111 - acc: 0.8795 - val_loss: 0.3218 - val_acc: 0.8726\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.87209 to 0.87257, saving model to ./cnn.2.h5\n",
      "Epoch 30/50\n",
      "45885/45885 [==============================] - 14s 295us/step - loss: 0.3107 - acc: 0.8799 - val_loss: 0.3221 - val_acc: 0.8719\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.87257\n",
      "Epoch 31/50\n",
      "45885/45885 [==============================] - 15s 330us/step - loss: 0.3101 - acc: 0.8793 - val_loss: 0.3209 - val_acc: 0.8730\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.87257 to 0.87297, saving model to ./cnn.2.h5\n",
      "Epoch 32/50\n",
      "45885/45885 [==============================] - 14s 298us/step - loss: 0.3090 - acc: 0.8802 - val_loss: 0.3217 - val_acc: 0.8724\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.87297\n",
      "Epoch 33/50\n",
      "45885/45885 [==============================] - 15s 328us/step - loss: 0.3083 - acc: 0.8803 - val_loss: 0.3208 - val_acc: 0.8728\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.87297\n",
      "Epoch 34/50\n",
      "45885/45885 [==============================] - 13s 294us/step - loss: 0.3091 - acc: 0.8800 - val_loss: 0.3224 - val_acc: 0.8716\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.87297\n",
      "Epoch 35/50\n",
      "45885/45885 [==============================] - 15s 330us/step - loss: 0.3068 - acc: 0.8805 - val_loss: 0.3198 - val_acc: 0.8725\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.87297\n",
      "Epoch 36/50\n",
      "45885/45885 [==============================] - 13s 287us/step - loss: 0.3067 - acc: 0.8811 - val_loss: 0.3188 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00036: val_acc improved from 0.87297 to 0.87328, saving model to ./cnn.2.h5\n",
      "Epoch 37/50\n",
      "45885/45885 [==============================] - 15s 330us/step - loss: 0.3054 - acc: 0.8812 - val_loss: 0.3187 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.87328 to 0.87363, saving model to ./cnn.2.h5\n",
      "Epoch 38/50\n",
      "45885/45885 [==============================] - 14s 299us/step - loss: 0.3053 - acc: 0.8815 - val_loss: 0.3188 - val_acc: 0.8734\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.87363\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45885/45885 [==============================] - 15s 325us/step - loss: 0.3066 - acc: 0.8810 - val_loss: 0.3189 - val_acc: 0.8739\n",
      "\n",
      "Epoch 00039: val_acc improved from 0.87363 to 0.87390, saving model to ./cnn.2.h5\n",
      "Epoch 40/50\n",
      "45885/45885 [==============================] - 13s 291us/step - loss: 0.3054 - acc: 0.8808 - val_loss: 0.3191 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00040: val_acc improved from 0.87390 to 0.87443, saving model to ./cnn.2.h5\n",
      "Epoch 41/50\n",
      "45885/45885 [==============================] - 15s 326us/step - loss: 0.3055 - acc: 0.8811 - val_loss: 0.3193 - val_acc: 0.8735\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.87443\n",
      "Epoch 42/50\n",
      "45885/45885 [==============================] - 13s 292us/step - loss: 0.3046 - acc: 0.8807 - val_loss: 0.3184 - val_acc: 0.8747\n",
      "\n",
      "Epoch 00042: val_acc improved from 0.87443 to 0.87465, saving model to ./cnn.2.h5\n",
      "Epoch 43/50\n",
      "45885/45885 [==============================] - 15s 327us/step - loss: 0.3047 - acc: 0.8816 - val_loss: 0.3172 - val_acc: 0.8739\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.87465\n",
      "Epoch 44/50\n",
      "45885/45885 [==============================] - 14s 306us/step - loss: 0.3043 - acc: 0.8824 - val_loss: 0.3179 - val_acc: 0.8742\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.87465\n",
      "Epoch 45/50\n",
      "45885/45885 [==============================] - 15s 330us/step - loss: 0.3039 - acc: 0.8820 - val_loss: 0.3174 - val_acc: 0.8746\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.87465\n",
      "Epoch 46/50\n",
      "45885/45885 [==============================] - 13s 290us/step - loss: 0.3039 - acc: 0.8820 - val_loss: 0.3168 - val_acc: 0.8749\n",
      "\n",
      "Epoch 00046: val_acc improved from 0.87465 to 0.87487, saving model to ./cnn.2.h5\n",
      "Epoch 47/50\n",
      "45885/45885 [==============================] - 15s 329us/step - loss: 0.3035 - acc: 0.8821 - val_loss: 0.3179 - val_acc: 0.8741\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.87487\n",
      "Epoch 48/50\n",
      "45885/45885 [==============================] - 13s 288us/step - loss: 0.3017 - acc: 0.8827 - val_loss: 0.3175 - val_acc: 0.8742\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.87487\n",
      "Epoch 49/50\n",
      "45885/45885 [==============================] - 15s 327us/step - loss: 0.3031 - acc: 0.8822 - val_loss: 0.3169 - val_acc: 0.8751\n",
      "\n",
      "Epoch 00049: val_acc improved from 0.87487 to 0.87509, saving model to ./cnn.2.h5\n",
      "Epoch 50/50\n",
      "45885/45885 [==============================] - 13s 286us/step - loss: 0.3024 - acc: 0.8820 - val_loss: 0.3176 - val_acc: 0.8749\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.87509\n"
     ]
    }
   ],
   "source": [
    "hist = cnn.fit(xs_train, ys_train,\n",
    "         batch_size=batch_size,\n",
    "         epochs=50,\n",
    "         validation_data=(xs_test, ys_test),\n",
    "         callbacks=[learning_rate_reduction,best_model,early_stopping],\n",
    "         verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's performing a bit better now, at 87.5%. This is satisfactory as a version 1 model, so let's move on to our MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our MLP will be straight-forward: an input layer, three hidden layers, and a sigmoid output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = Sequential()\n",
    "\n",
    "mlp.add(Dense(512, input_shape=(8,)))\n",
    "mlp.add(LeakyReLU())\n",
    "mlp.add(Dropout(.5))\n",
    "\n",
    "mlp.add(Dense(1024))\n",
    "mlp.add(LeakyReLU())\n",
    "mlp.add(Dropout(.5))\n",
    "\n",
    "mlp.add(Dense(1024))\n",
    "mlp.add(LeakyReLU())\n",
    "mlp.add(Dropout(.5))\n",
    "\n",
    "mlp.add(Dense(512))\n",
    "mlp.add(LeakyReLU())\n",
    "mlp.add(Dropout(.5))\n",
    "\n",
    "mlp.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "mlp.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "             metrics=['acc'])\n",
    "\n",
    "learning_rate_reduction_mlp = ReduceLROnPlateau(monitor='val_acc', patience=3, \n",
    "                                            verbose=2, factor=0.5, min_lr=0.00001)\n",
    "\n",
    "best_model_mlp = ModelCheckpoint('./mlp.1.h5', monitor='val_acc', verbose=2, \n",
    "                             save_best_only=True, mode='max')\n",
    "\n",
    "early_stopping_mlp = EarlyStopping(monitor='val_loss', min_delta=1e-10, \n",
    "                               patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45885 samples, validate on 22601 samples\n",
      "Epoch 1/5\n",
      "45885/45885 [==============================] - 17s 367us/step - loss: 0.4153 - acc: 0.8612 - val_loss: 0.3880 - val_acc: 0.8661\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.86607, saving model to ./mlp.1.h5\n",
      "Epoch 2/5\n",
      "45885/45885 [==============================] - 14s 314us/step - loss: 0.4068 - acc: 0.8630 - val_loss: 0.3996 - val_acc: 0.8662\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.86607 to 0.86624, saving model to ./mlp.1.h5\n",
      "Epoch 3/5\n",
      "45885/45885 [==============================] - 15s 337us/step - loss: 0.4071 - acc: 0.8630 - val_loss: 0.3928 - val_acc: 0.8660\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.86624\n",
      "Epoch 4/5\n",
      "45885/45885 [==============================] - 15s 316us/step - loss: 0.4059 - acc: 0.8628 - val_loss: 0.3868 - val_acc: 0.8658\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.86624\n",
      "Epoch 5/5\n",
      "45885/45885 [==============================] - 15s 321us/step - loss: 0.4054 - acc: 0.8626 - val_loss: 0.3885 - val_acc: 0.8646\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.86624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa939ac4bd0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(xn_train, yn_train,\n",
    "         batch_size=batch_size,\n",
    "         epochs=5,\n",
    "         validation_data=(xn_test, yn_test),\n",
    "         callbacks=[learning_rate_reduction_mlp,best_model_mlp,early_stopping_mlp],\n",
    "         verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is performing about as well as the simple logistic regressor.\n",
    "\n",
    "Let's combine our two models and see if the two parts form a stronger whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Model\n",
    "\n",
    "The architecture of our hybrid model will be as follows: we will have our sequence data go into training a 1D CNN, and our numerical and categorical data will go into training an MLP; each will have a final dense layer of 64 outputs, these outputs will flow into a final hybrid MLP with four hidden dense layers and a sigmoid output layer.\n",
    "\n",
    "Let's build the model and see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "\n",
    "    # cnn\n",
    "    seq_input = Input(shape=(max_length,))\n",
    "\n",
    "    x = Embedding(vocab_size, 3, input_length=max_length)(seq_input)\n",
    "\n",
    "    x = Conv1D(256, kernel_size=3, strides=1)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(1024)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(.5)(x)\n",
    "\n",
    "    x = Dense(1024)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(.5)(x)\n",
    "\n",
    "    x = Dense(512)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(.5)(x)\n",
    "    \n",
    "    x = Dense(512)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(.5)(x)\n",
    "\n",
    "    seq_output = Dense(64, activation='relu')(x)\n",
    "\n",
    "    cnn = Model(inputs=seq_input, outputs=seq_output)\n",
    "\n",
    "    # mlp\n",
    "    num_input = Input(shape=(8,))\n",
    "    y = Dense(512)(num_input)\n",
    "    y = LeakyReLU()(y)\n",
    "    y = Dropout(.5)(y)\n",
    "\n",
    "    y = Dense(1024)(y)\n",
    "    y = LeakyReLU()(y)\n",
    "    y = Dropout(.5)(y)\n",
    "\n",
    "    y = Dense(1024)(y)\n",
    "    y = LeakyReLU()(y)\n",
    "    y = Dropout(.5)(y)\n",
    "\n",
    "    y = Dense(512)(y)\n",
    "    y = LeakyReLU()(y)\n",
    "    y = Dropout(.5)(y)\n",
    "\n",
    "    mlp_output = Dense(64, activation='relu')(y)\n",
    "\n",
    "    mlp = Model(inputs=num_input, outputs=mlp_output)\n",
    "\n",
    "    # combine\n",
    "    combined = Concatenate()([cnn.output, mlp.output])\n",
    "    \n",
    "    z = Dense(512)(combined)\n",
    "    z = LeakyReLU()(z)\n",
    "    z = Dropout(.5)(z)\n",
    "    \n",
    "    z = Dense(512)(z)\n",
    "    z = LeakyReLU()(z)\n",
    "    z = Dropout(.2)(z)\n",
    "    \n",
    "    z = Dense(256)(z)\n",
    "    z = LeakyReLU()(z)\n",
    "    z = Dropout(.2)(z)\n",
    "    \n",
    "    z = Dense(64)(z)\n",
    "    z = LeakyReLU()(z)\n",
    "    z = Dropout(.2)(z)\n",
    "    \n",
    "    output = Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "    final_model = Model(inputs=mlp.inputs + cnn.inputs, outputs=[output])\n",
    "\n",
    "    final_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = x_train[:, :8]\n",
    "seq_train = x_train[:, 8:]\n",
    "\n",
    "num_test = x_test[:, :8]\n",
    "seq_test = x_test[:, 8:]\n",
    "\n",
    "xc_train = [num_train, seq_train] \n",
    "xc_test = [num_test, seq_test] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction_combined = ReduceLROnPlateau(monitor='val_acc', patience=3, \n",
    "                                            verbose=2, factor=0.5, min_lr=0.00001)\n",
    "\n",
    "best_model_combined = ModelCheckpoint('./combined_cnn_mlp_model.1.4.h5', monitor='val_acc', verbose=2, \n",
    "                             save_best_only=True, mode='max')\n",
    "\n",
    "early_stopping_combined = EarlyStopping(monitor='val_loss', min_delta=1e-10, \n",
    "                               patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45885 samples, validate on 22601 samples\n",
      "Epoch 1/50\n",
      "45885/45885 [==============================] - 37s 805us/step - loss: 0.3870 - acc: 0.8646 - val_loss: 0.3407 - val_acc: 0.8843\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.88430, saving model to ./combined_cnn_mlp_model.2.h5\n",
      "Epoch 2/50\n",
      "45885/45885 [==============================] - 30s 645us/step - loss: 0.3272 - acc: 0.8851 - val_loss: 0.2919 - val_acc: 0.8937\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.88430 to 0.89372, saving model to ./combined_cnn_mlp_model.2.h5\n",
      "Epoch 3/50\n",
      "45885/45885 [==============================] - 29s 634us/step - loss: 0.3166 - acc: 0.8910 - val_loss: 0.2982 - val_acc: 0.8921\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.89372\n",
      "Epoch 4/50\n",
      "45885/45885 [==============================] - 29s 633us/step - loss: 0.3087 - acc: 0.8929 - val_loss: 0.3149 - val_acc: 0.8990\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.89372 to 0.89899, saving model to ./combined_cnn_mlp_model.2.h5\n",
      "Epoch 5/50\n",
      "45885/45885 [==============================] - 30s 647us/step - loss: 0.3113 - acc: 0.8935 - val_loss: 0.3122 - val_acc: 0.8986\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.89899\n",
      "Epoch 6/50\n",
      "45885/45885 [==============================] - 29s 639us/step - loss: 0.3092 - acc: 0.8936 - val_loss: 0.2895 - val_acc: 0.8990\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.89899\n",
      "Epoch 7/50\n",
      "45885/45885 [==============================] - 30s 655us/step - loss: 0.3069 - acc: 0.8942 - val_loss: 0.3015 - val_acc: 0.8940\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.89899\n",
      "Epoch 8/50\n",
      "45885/45885 [==============================] - 30s 658us/step - loss: 0.2861 - acc: 0.8997 - val_loss: 0.2861 - val_acc: 0.9039\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.89899 to 0.90385, saving model to ./combined_cnn_mlp_model.2.h5\n",
      "Epoch 9/50\n",
      "45885/45885 [==============================] - 30s 658us/step - loss: 0.2823 - acc: 0.9014 - val_loss: 0.2700 - val_acc: 0.9033\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.90385\n",
      "Epoch 10/50\n",
      "45885/45885 [==============================] - 30s 661us/step - loss: 0.2801 - acc: 0.9018 - val_loss: 0.2886 - val_acc: 0.9038\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.90385\n",
      "Epoch 11/50\n",
      "45885/45885 [==============================] - 31s 684us/step - loss: 0.2788 - acc: 0.9031 - val_loss: 0.2769 - val_acc: 0.9050\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.90385 to 0.90496, saving model to ./combined_cnn_mlp_model.2.h5\n",
      "Epoch 12/50\n",
      "45885/45885 [==============================] - 30s 658us/step - loss: 0.2797 - acc: 0.9019 - val_loss: 0.2796 - val_acc: 0.9034\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.90496\n",
      "Epoch 13/50\n",
      "45885/45885 [==============================] - 31s 679us/step - loss: 0.2777 - acc: 0.9035 - val_loss: 0.2716 - val_acc: 0.9027\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.90496\n",
      "Epoch 14/50\n",
      "45885/45885 [==============================] - 30s 654us/step - loss: 0.2752 - acc: 0.9032 - val_loss: 0.2730 - val_acc: 0.9050\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.90496 to 0.90505, saving model to ./combined_cnn_mlp_model.2.h5\n",
      "Epoch 15/50\n",
      "45885/45885 [==============================] - 32s 696us/step - loss: 0.2649 - acc: 0.9054 - val_loss: 0.2674 - val_acc: 0.9061\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.90505 to 0.90607, saving model to ./combined_cnn_mlp_model.2.h5\n",
      "Epoch 16/50\n",
      "45885/45885 [==============================] - 30s 657us/step - loss: 0.2648 - acc: 0.9071 - val_loss: 0.2612 - val_acc: 0.9081\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.90607 to 0.90815, saving model to ./combined_cnn_mlp_model.2.h5\n",
      "Epoch 17/50\n",
      "45885/45885 [==============================] - 21s 455us/step - loss: 0.2614 - acc: 0.9080 - val_loss: 0.2610 - val_acc: 0.9083\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.90815 to 0.90828, saving model to ./combined_cnn_mlp_model.2.h5\n",
      "Epoch 18/50\n",
      "45885/45885 [==============================] - 21s 457us/step - loss: 0.2610 - acc: 0.9077 - val_loss: 0.2630 - val_acc: 0.9077\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.90828\n",
      "Epoch 19/50\n",
      "45885/45885 [==============================] - 21s 450us/step - loss: 0.2594 - acc: 0.9075 - val_loss: 0.2668 - val_acc: 0.9058\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.90828\n",
      "Epoch 20/50\n",
      "45885/45885 [==============================] - 20s 443us/step - loss: 0.2599 - acc: 0.9086 - val_loss: 0.2605 - val_acc: 0.9078\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.90828\n",
      "Epoch 21/50\n",
      "45885/45885 [==============================] - 21s 450us/step - loss: 0.2533 - acc: 0.9094 - val_loss: 0.2594 - val_acc: 0.9087\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.90828 to 0.90872, saving model to ./combined_cnn_mlp_model.2.h5\n",
      "Epoch 22/50\n",
      "45885/45885 [==============================] - 21s 448us/step - loss: 0.2512 - acc: 0.9099 - val_loss: 0.2588 - val_acc: 0.9094\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.90872 to 0.90943, saving model to ./combined_cnn_mlp_model.2.h5\n",
      "Epoch 23/50\n",
      "45885/45885 [==============================] - 21s 455us/step - loss: 0.2510 - acc: 0.9104 - val_loss: 0.2626 - val_acc: 0.9068\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.90943\n",
      "Epoch 24/50\n",
      "45885/45885 [==============================] - 21s 457us/step - loss: 0.2528 - acc: 0.9099 - val_loss: 0.2625 - val_acc: 0.9093\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.90943\n",
      "Epoch 25/50\n",
      "45885/45885 [==============================] - 21s 459us/step - loss: 0.2518 - acc: 0.9108 - val_loss: 0.2596 - val_acc: 0.9080\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.90943\n",
      "Epoch 26/50\n",
      "45885/45885 [==============================] - 20s 446us/step - loss: 0.2477 - acc: 0.9118 - val_loss: 0.2578 - val_acc: 0.9078\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.90943\n",
      "Epoch 27/50\n",
      "45885/45885 [==============================] - 21s 460us/step - loss: 0.2463 - acc: 0.9116 - val_loss: 0.2563 - val_acc: 0.9092\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.90943\n",
      "Epoch 28/50\n",
      "45885/45885 [==============================] - 21s 453us/step - loss: 0.2469 - acc: 0.9118 - val_loss: 0.2567 - val_acc: 0.9093\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.90943\n",
      "Epoch 29/50\n",
      "45885/45885 [==============================] - 20s 441us/step - loss: 0.2445 - acc: 0.9113 - val_loss: 0.2552 - val_acc: 0.9094\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.90943\n",
      "Epoch 30/50\n",
      "45885/45885 [==============================] - 21s 465us/step - loss: 0.2442 - acc: 0.9121 - val_loss: 0.2557 - val_acc: 0.9095\n",
      "\n",
      "Epoch 00030: val_acc improved from 0.90943 to 0.90952, saving model to ./combined_cnn_mlp_model.2.h5\n",
      "Epoch 31/50\n",
      "45885/45885 [==============================] - 20s 445us/step - loss: 0.2426 - acc: 0.9128 - val_loss: 0.2559 - val_acc: 0.9098\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.90952 to 0.90983, saving model to ./combined_cnn_mlp_model.2.h5\n",
      "Epoch 32/50\n",
      "45885/45885 [==============================] - 20s 442us/step - loss: 0.2429 - acc: 0.9123 - val_loss: 0.2548 - val_acc: 0.9091\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.90983\n",
      "Epoch 33/50\n",
      "45885/45885 [==============================] - 20s 445us/step - loss: 0.2438 - acc: 0.9121 - val_loss: 0.2549 - val_acc: 0.9097\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.90983\n",
      "Epoch 34/50\n",
      "45885/45885 [==============================] - 20s 441us/step - loss: 0.2429 - acc: 0.9122 - val_loss: 0.2560 - val_acc: 0.9091\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.90983\n",
      "Epoch 35/50\n",
      "45885/45885 [==============================] - 20s 441us/step - loss: 0.2421 - acc: 0.9126 - val_loss: 0.2556 - val_acc: 0.9090\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.90983\n",
      "Epoch 36/50\n",
      "45885/45885 [==============================] - 20s 443us/step - loss: 0.2418 - acc: 0.9127 - val_loss: 0.2550 - val_acc: 0.9088\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.90983\n",
      "Epoch 37/50\n",
      "45885/45885 [==============================] - 20s 444us/step - loss: 0.2411 - acc: 0.9126 - val_loss: 0.2555 - val_acc: 0.9091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.90983\n",
      "Epoch 38/50\n",
      "45885/45885 [==============================] - 21s 451us/step - loss: 0.2417 - acc: 0.9118 - val_loss: 0.2542 - val_acc: 0.9093\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.90983\n",
      "Epoch 39/50\n",
      "45885/45885 [==============================] - 21s 454us/step - loss: 0.2412 - acc: 0.9125 - val_loss: 0.2542 - val_acc: 0.9094\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.90983\n",
      "Epoch 40/50\n",
      "45885/45885 [==============================] - 21s 447us/step - loss: 0.2415 - acc: 0.9128 - val_loss: 0.2543 - val_acc: 0.9090\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.90983\n",
      "Epoch 41/50\n",
      "45885/45885 [==============================] - 21s 454us/step - loss: 0.2418 - acc: 0.9129 - val_loss: 0.2541 - val_acc: 0.9094\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.90983\n",
      "Epoch 42/50\n",
      "45885/45885 [==============================] - 21s 451us/step - loss: 0.2422 - acc: 0.9131 - val_loss: 0.2542 - val_acc: 0.9101\n",
      "\n",
      "Epoch 00042: val_acc improved from 0.90983 to 0.91009, saving model to ./combined_cnn_mlp_model.2.h5\n",
      "Epoch 43/50\n",
      "45885/45885 [==============================] - 21s 452us/step - loss: 0.2412 - acc: 0.9128 - val_loss: 0.2543 - val_acc: 0.9093\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.91009\n",
      "Epoch 44/50\n",
      "45885/45885 [==============================] - 21s 456us/step - loss: 0.2404 - acc: 0.9132 - val_loss: 0.2544 - val_acc: 0.9093\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.91009\n",
      "Epoch 45/50\n",
      "45885/45885 [==============================] - 21s 450us/step - loss: 0.2421 - acc: 0.9122 - val_loss: 0.2541 - val_acc: 0.9094\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.91009\n",
      "Epoch 46/50\n",
      "45885/45885 [==============================] - 21s 447us/step - loss: 0.2409 - acc: 0.9127 - val_loss: 0.2544 - val_acc: 0.9093\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.91009\n",
      "Epoch 47/50\n",
      "45885/45885 [==============================] - 21s 463us/step - loss: 0.2401 - acc: 0.9130 - val_loss: 0.2540 - val_acc: 0.9093\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.91009\n",
      "Epoch 48/50\n",
      "45885/45885 [==============================] - 21s 455us/step - loss: 0.2414 - acc: 0.9124 - val_loss: 0.2541 - val_acc: 0.9092\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.91009\n",
      "Epoch 49/50\n",
      "45885/45885 [==============================] - 21s 451us/step - loss: 0.2390 - acc: 0.9131 - val_loss: 0.2546 - val_acc: 0.9089\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.91009\n",
      "Epoch 50/50\n",
      "45885/45885 [==============================] - 21s 451us/step - loss: 0.2411 - acc: 0.9129 - val_loss: 0.2544 - val_acc: 0.9089\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.91009\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(xc_train, y_train,\n",
    "         batch_size=batch_size,\n",
    "         epochs=50,\n",
    "         validation_data=(xc_test, y_test),\n",
    "         callbacks = [learning_rate_reduction_combined, best_model_combined, early_stopping_combined],\n",
    "         verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance was improved by a lot! The individual CNN and MLP models were scoring around 86% accuracy, and our hybrid model scored 91% accuracy.\n",
    "\n",
    "Next, we will try tweaking the inputs for model_def version 2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
